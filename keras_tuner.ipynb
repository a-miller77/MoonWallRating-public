{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9542c8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "import pickle\n",
    "from sklearn.utils import class_weight\n",
    "from model_definition import *\n",
    "from preprocessing import vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6ee31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_preprocessed_routes', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f).values()\n",
    "#with open('./data/test_preprocessed_routes', 'rb') as f:\n",
    "#    X_test, y_test = pickle.load(f).values()\n",
    "with open('./data/val_preprocessed_routes', 'rb') as f:\n",
    "    X_val, y_val = pickle.load(f).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7731334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, 0] = 3\n",
    "#X_test[:, 0] = 3\n",
    "X_val[:, 0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3440d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables:\n",
    "\n",
    "#constant\n",
    "num_classes = 9\n",
    "epochs = 30\n",
    "\n",
    "# Iteration one, 68 trials :\n",
    "\n",
    "# num_layers   =  2    -  8,   step 2   ->   2\n",
    "# d_model      =  64   -  512, step 64  ->   64\n",
    "# dff          =  512  -  2048, step 256 ->  768\n",
    "# num_heads    =  4    -  10,   step 2   ->  10\n",
    "# dropout_rate =  0.1  -  0.4,  step 0.1 ->  0.1\n",
    "# warmup_steps =  2500 -  5500, step 500 ->  3500\n",
    "\n",
    "\n",
    "\n",
    "# Iteration two, 100 trials:\n",
    "\n",
    "# num_layers   =  1    -  4,   step 1   ->  3\n",
    "# d_model      =  16   -  128, step 8  -> 128\n",
    "# dff          =  512  -  1024, step 32 -> 736\n",
    "# num_heads    =  8    -  14,   step 1   -> 8\n",
    "# dropout_rate =  0.04 -  0.24,  step 0.02 -> 0.2\n",
    "# warmup_steps =  2500 -  4750, step 250 -> 3250\n",
    "# beta_1       =  0.79 -  0.95, step 0.02 -> 0.79\n",
    "# beta_2       =  0.95 -  0.99, step 0.005 -> 0.98\n",
    "# epsilon      = 1e-11 -  1e-7, step NA   -> 6.35e-08\n",
    "\n",
    "# Iteration three, 99 trials:\n",
    "\n",
    "# num_layers   =  1    -  4,   step 1   ->  \n",
    "    # default: 2\n",
    "# d_model      =  16   -  192, step 16  -> \n",
    "    # default: 128\n",
    "# dff          =  512  -  1280, step 64 ->\n",
    "    # default: 768\n",
    "# num_heads    =  8    -  14,   step 1   -> \n",
    "    # default: 10\n",
    "# dropout_rate =  0.125 -  0.30,  step 0.025 -> \n",
    "    # default: 0.2\n",
    "# warmup_steps =  2000 -  7000, step 500 -> \n",
    "    # default: 4000\n",
    "# beta_1       =  0.74 -  0.93, step 0.0025 -> \n",
    "    # default: 0.8\n",
    "# beta_2       =  0.95 -  0.99, step 0.005 -> \n",
    "    # default: 0.98\n",
    "# epsilon      = 1e-11 -  1e-7, sample 'log' ->\n",
    "    # default: 1e-8\n",
    "# global_batch =  16   -  128,  step 16  ->\n",
    "    # default: 64\n",
    "\n",
    "\n",
    "# Implement:\n",
    "# global_batch =  16   -  128,  step 16  ->\n",
    "\n",
    "# learning_rate = CustomSchedule(d_model, warmup_steps=warmup_steps)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0255abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp): \n",
    "        num_layers = hp.Int(\"num_layers\", min_value=1, max_value=4, default=2, step=1)\n",
    "        d_model = hp.Int(\"d_model\", min_value=16, max_value=192, default=128, step=16)\n",
    "        dff = hp.Int(\"dff\", min_value=512, max_value=1280, default=768, step=64)\n",
    "        num_heads = hp.Int(\"num_heads\", min_value=8, max_value=14, default=10, step=1)\n",
    "        dropout_rate = hp.Float(\"dropout_rate\", min_value=0.125, max_value=0.3, default=0.2, step=0.025)\n",
    "        \n",
    "        warmup_steps = hp.Int(\"warmup_steps\", min_value=2000, max_value=7000, default=4000, step=500)\n",
    "        batch_size = hp.Int(\"batch_size\", min_value=16, max_value=128, step=16)\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.74, max_value=0.93, default= .8, step=0.0025)\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.95, max_value=0.99, default= .98, step=0.005)\n",
    "        epsilon = hp.Float(\"epsilon\", min_value=1e-9, max_value=5e-7, default= 1e-8, sampling=\"log\")\n",
    "    \n",
    "        num_classes = 9\n",
    "\n",
    "        model = EncoderClassifier(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            vocab_size=vocab_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=hp.Choice(\"activation\", values=['relu', 'swish'], default='relu'),\n",
    "            sequential=hp.Boolean(\"sequential\")\n",
    "        )\n",
    "        \n",
    "        learning_rate = CustomSchedule(d_model, warmup_steps=warmup_steps)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, \n",
    "                                beta_1=hp.Float(\"beta_1\", min_value=0.74, max_value=0.93, default= .8, step=0.0025),\n",
    "                                beta_2=hp.Float(\"beta_2\", min_value=0.95, max_value=0.99, default= .98, step=0.005),\n",
    "                                epsilon=hp.Float(\"epsilon\", min_value=1e-9, max_value=5e-7, default= 1e-8, sampling=\"log\"))\n",
    "        \n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, x, y, epochs, validation_data, verbose=1, **kwargs):\n",
    "        batch_size = hp.Int(\"batch_size\", 16, 128, step=16, default=64)\n",
    "        \n",
    "        # Convert the datasets to tf.data.Dataset.        \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "        validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "\n",
    "        history = model.fit(train_ds, epochs=epochs, validation_data=validation_data, verbose=1,\n",
    "                  class_weight=class_weights)\n",
    "        \n",
    "        #step = np.argmax(history.history['val_accuracy'])\n",
    "        #loss = history.history['val_accuracy'][step]\n",
    "        #accuracy = history.history['val_accuracy'][step]\n",
    "        #val_loss = history.history['val_accuracy'][step]\n",
    "        #val_accuracy = history.history['val_accuracy'][step]\n",
    "           \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197ec1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from tuners/tuner3/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner_name = \"tuner3\"\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    #hyperparameters=hp,\n",
    "    #tune_new_entries=True,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,\n",
    "    overwrite=False,\n",
    "    directory=\"tuners\",\n",
    "    project_name=tuner_name,\n",
    ")\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(y_train),\n",
    "                                                  y=y_train) \n",
    "class_weights=dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "\n",
    "my_callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.05, patience=3,\n",
    "    verbose=2, baseline=0.40, start_from_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38171e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |?                 |num_layers\n",
      "80                |?                 |d_model\n",
      "832               |?                 |dff\n",
      "12                |?                 |num_heads\n",
      "0.175             |?                 |dropout_rate\n",
      "4500              |?                 |warmup_steps\n",
      "128               |?                 |batch_size\n",
      "0.91              |?                 |beta_1\n",
      "0.985             |?                 |beta_2\n",
      "2.7038e-09        |?                 |epsilon\n",
      "relu              |?                 |activation\n",
      "True              |?                 |sequential\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 16:46:39.502036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-17 16:46:41.265546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13641 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:60:00.0, compute capability: 7.5\n",
      "2023-03-17 16:46:41.268571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13641 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:61:00.0, compute capability: 7.5\n",
      "2023-03-17 16:46:48.328922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-03-17 16:46:48.743457: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x27171330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-17 16:46:48.743510: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-03-17 16:46:48.743520: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "2023-03-17 16:46:48.769002: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-17 16:46:49.113007: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 17s 44ms/step - loss: 2.2685 - accuracy: 0.1293 - val_loss: 2.1455 - val_accuracy: 0.2222\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 3s 22ms/step - loss: 2.0915 - accuracy: 0.1683 - val_loss: 1.7172 - val_accuracy: 0.2513\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 3s 22ms/step - loss: 1.6469 - accuracy: 0.2883 - val_loss: 1.5037 - val_accuracy: 0.3813\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 3s 22ms/step - loss: 1.4935 - accuracy: 0.3649 - val_loss: 1.3746 - val_accuracy: 0.4458\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 3s 23ms/step - loss: 1.4203 - accuracy: 0.3909 - val_loss: 1.4050 - val_accuracy: 0.4242\n"
     ]
    },
    {
     "ename": "FatalTypeError",
     "evalue": "Expected the return value of HyperModel.fit() to be one of float, dict, keras.callbacks.History, or a list of one of these types. Recevied return value: {<keras.callbacks.History object at 0x7f34a2fdb850>} of type <class 'set'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalTypeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:226\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:271\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, errors\u001b[38;5;241m.\u001b[39mFatalError):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_module\u001b[38;5;241m.\u001b[39mDEBUG:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;66;03m# Printing the stacktrace and the error.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:266\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/base_tuner.py:231\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 231\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    234\u001b[0m     ):\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    247\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py:287\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    286\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 287\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner.py:215\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m    214\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 215\u001b[0m \u001b[43mtuner_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_trial_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHyperModel.fit()\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras_tuner/engine/tuner_utils.py:333\u001b[0m, in \u001b[0;36mvalidate_trial_results\u001b[0;34m(results, objective, func_name)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# Other unsupported types.\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mFatalTypeError(\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the return value of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone of float, dict, keras.callbacks.History, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a list of one of these types. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecevied return value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m )\n",
      "\u001b[0;31mFatalTypeError\u001b[0m: Expected the return value of HyperModel.fit() to be one of float, dict, keras.callbacks.History, or a list of one of these types. Recevied return value: {<keras.callbacks.History object at 0x7f34a2fdb850>} of type <class 'set'>."
     ]
    }
   ],
   "source": [
    "tuner.search(x=X_train, y=y_train, epochs=5, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights, callbacks=[my_callbacks])\n",
    "#best_model = tuner.get_best_models()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
