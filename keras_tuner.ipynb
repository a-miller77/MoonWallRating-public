{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9542c8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "import pickle\n",
    "from sklearn.utils import class_weight\n",
    "from model_definition import *\n",
    "from tuner_trial_functions import *\n",
    "from preprocessing import vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c6ee31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_preprocessed_routes', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f).values()\n",
    "#with open('./data/test_preprocessed_routes', 'rb') as f:\n",
    "#    X_test, y_test = pickle.load(f).values()\n",
    "with open('./data/val_preprocessed_routes', 'rb') as f:\n",
    "    X_val, y_val = pickle.load(f).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7731334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, 0] = 3\n",
    "#X_test[:, 0] = 3\n",
    "X_val[:, 0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5a2c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_num = 0 #initialization\n",
    "hp_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3440d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables:\n",
    "\n",
    "#constant\n",
    "num_classes = 9\n",
    "epochs = 30\n",
    "\n",
    "# Iteration one, 68 trials :\n",
    "\n",
    "# num_layers   =  2    -  8,   step 2   ->   2\n",
    "# d_model      =  64   -  512, step 64  ->   64\n",
    "# dff          =  512  -  2048, step 256 ->  768\n",
    "# num_heads    =  4    -  10,   step 2   ->  10\n",
    "# dropout_rate =  0.1  -  0.4,  step 0.1 ->  0.1\n",
    "# warmup_steps =  2500 -  5500, step 500 ->  3500\n",
    "\n",
    "\n",
    "\n",
    "# Iteration two, 100 trials:\n",
    "\n",
    "# num_layers   =  1    -  4,   step 1   ->  3\n",
    "# d_model      =  16   -  128, step 8  -> 128\n",
    "# dff          =  512  -  1024, step 32 -> 736\n",
    "# num_heads    =  8    -  14,   step 1   -> 8\n",
    "# dropout_rate =  0.04 -  0.24,  step 0.02 -> 0.2\n",
    "# warmup_steps =  2500 -  4750, step 250 -> 3250\n",
    "# beta_1       =  0.79 -  0.95, step 0.02 -> 0.79\n",
    "# beta_2       =  0.95 -  0.99, step 0.005 -> 0.98\n",
    "# epsilon      = 1e-11 -  1e-7, step NA   -> 6.35e-08\n",
    "\n",
    "# Iteration three, 99 trials:\n",
    "\n",
    "# num_layers   =  1    -  4,   step 1   ->  \n",
    "    # default: 2\n",
    "# d_model      =  16   -  192, step 16  -> \n",
    "    # default: 128\n",
    "# dff          =  512  -  1280, step 64 ->\n",
    "    # default: 768\n",
    "# num_heads    =  8    -  14,   step 1   -> \n",
    "    # default: 10\n",
    "# dropout_rate =  0.125 -  0.30,  step 0.025 -> \n",
    "    # default: 0.2\n",
    "# warmup_steps =  2000 -  7000, step 500 -> \n",
    "    # default: 4000\n",
    "# beta_1       =  0.74 -  0.93, step 0.0025 -> \n",
    "    # default: 0.8\n",
    "# beta_2       =  0.95 -  0.99, step 0.005 -> \n",
    "    # default: 0.98\n",
    "# epsilon      = 1e-11 -  1e-7, sample 'log' ->\n",
    "    # default: 1e-8\n",
    "# global_batch =  16   -  128,  step 16  ->\n",
    "    # default: 64\n",
    "\n",
    "\n",
    "# Implement:\n",
    "# global_batch =  16   -  128,  step 16  ->\n",
    "\n",
    "# learning_rate = CustomSchedule(d_model, warmup_steps=warmup_steps)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0255abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):     \n",
    "        num_classes = 9\n",
    "        self.batch_size = hp.get('batch_size')\n",
    "\n",
    "        model = EncoderClassifier(\n",
    "            num_layers=hp.get('num_layers'),\n",
    "            d_model=hp.get('d_model'),\n",
    "            num_heads=hp.get('num_heads'),\n",
    "            dff=hp.get('dff'),\n",
    "            vocab_size=vocab_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=hp.get('dropout_rate'),\n",
    "            activation=hp.get('activation'),\n",
    "            sequential=hp.get('sequential')\n",
    "        )\n",
    "        \n",
    "        learning_rate = CustomSchedule(hp.get('d_model'), warmup_steps=hp.get('warmup_steps'))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, \n",
    "                                beta_1=hp.get(\"beta_1\"),\n",
    "                                beta_2=hp.get(\"beta_2\"),\n",
    "                                epsilon=hp.get(\"epsilon\"))\n",
    "        \n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, x, y, epochs, validation_data, verbose=1, **kwargs):\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        # Convert the datasets to tf.data.Dataset.        \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "        validation_data = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "\n",
    "        history = model.fit(train_ds, epochs=epochs, validation_data=validation_data, verbose=1,\n",
    "                  class_weight=class_weights)\n",
    "           \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "875af516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hp(hp, hp_type=hp_type, tuner_num=tuner_num):\n",
    "    trials_np = np.zeros(0)\n",
    "    base_dir = f'tuners/tuner{tuner_num}'\n",
    "    num_trials = get_num_trials(base_dir)\n",
    "    trials_np = create_trials_np(num_trials, base_dir)\n",
    "\n",
    "    tuner_df = create_tuner_df(num_trials, trials_np)\n",
    "    df = best_trials(tuner_df, num_trials=1)    \n",
    "    \n",
    "    hp.Fixed(\"Tuning_Mode\", value=hp_type)\n",
    "    \n",
    "    if(hp_type == \"layer\"):\n",
    "        # These Hyperparameters affect the size/complexity of the model layers\n",
    "        hp.Fixed(\"warmup_steps\", value=df['warmup_steps'].iloc[0])\n",
    "        hp.Fixed(\"batch_size\", value=df['batch_size'].iloc[0])\n",
    "        hp.Fixed(\"beta_1\", value=df['beta_1'].iloc[0])\n",
    "        hp.Fixed(\"beta_2\", value=df['beta_2'].iloc[0])\n",
    "        hp.Fixed(\"epsilon\", value=df['epsilon'].iloc[0])\n",
    "        \n",
    "        hp.Fixed(\"activation\", value='relu')\n",
    "        hp.Fixed(\"sequential\", value=True)\n",
    "    elif(hp_type == \"learn\"):\n",
    "        # These Hyperparameters affect the learning rate\n",
    "        hp.Fixed(\"num_layers\", value=df['num_layers'].iloc[0])\n",
    "        hp.Fixed(\"d_model\", value=df['d_model'].iloc[0])\n",
    "        hp.Fixed(\"dff\", value=df['dff'].iloc[0])\n",
    "        hp.Fixed(\"num_heads\", value=df['num_heads'].iloc[0])\n",
    "        hp.Fixed(\"dropout_rate\", value=df['dropout_rate'].iloc[0])\n",
    "        \n",
    "        hp.Fixed(\"activation\", value='relu')\n",
    "        hp.Fixed(\"sequential\", value=True)\n",
    "    else:\n",
    "        hp.Choice(\"activation\", values=['relu', 'swish'], default='relu')\n",
    "        hp.Boolean(\"sequential\", default=True)\n",
    "\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e18678a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hp(hp=keras_tuner.HyperParameters(), hp_type=hp_type, tuner_num=tuner_num):  \n",
    "    hp.Fixed(\"Tuning_Mode\", value=hp_type)\n",
    "    if(hp_type == \"layer\" or hp_type == \"base\"):\n",
    "        # These Hyperparameters affect the size/complexity of the model layers\n",
    "        num_layers = hp.Int(\"num_layers\", min_value=1, max_value=4, default=2, step=1)\n",
    "        d_model = hp.Int(\"d_model\", min_value=16, max_value=192, default=128, step=16)\n",
    "        dff = hp.Int(\"dff\", min_value=512, max_value=1280, default=768, step=64)\n",
    "        num_heads = hp.Int(\"num_heads\", min_value=8, max_value=14, default=10, step=1)\n",
    "        dropout_rate = hp.Float(\"dropout_rate\", min_value=0.125, max_value=0.3, default=0.2, step=0.025)\n",
    "        \n",
    "    if(hp_type == \"learn\" or hp_type == \"base\"):\n",
    "        # These Hyperparameters affect the learning rate\n",
    "        warmup_steps = hp.Int(\"warmup_steps\", min_value=2000, max_value=7000, default=4000, step=500)\n",
    "        batch_size = hp.Int(\"batch_size\", min_value=16, max_value=128, step=16)\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.74, max_value=0.93, default= .8, step=0.0025)\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.95, max_value=0.99, default= .98, step=0.005)\n",
    "        epsilon = hp.Float(\"epsilon\", min_value=1e-9, max_value=5e-7, default= 1e-8, sampling=\"log\")\n",
    "        \n",
    "    if(hp_type == \"arch\" or hp_type == \"base\"):\n",
    "        hp.Choice(\"activation\", values=['relu', 'swish'], default='relu')\n",
    "        hp.Boolean(\"sequential\", default=True)\n",
    "        \n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3fab4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner(hp, tune_new=True, trials=10, tuner_num=tuner_num, update=False, hp_type=hp_type):\n",
    "    if(update):\n",
    "        hp = update_hp(hp)\n",
    "    return keras_tuner.BayesianOptimization(\n",
    "        hypermodel=MyHyperModel(),\n",
    "        hyperparameters = hp,\n",
    "        tune_new_entries = tune_new,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=trials,\n",
    "        overwrite=False,\n",
    "        directory=\"tuners\",\n",
    "        project_name=f'tuner{tuner_num}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc5edb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(y_train),\n",
    "                                                  y=y_train) \n",
    "class_weights=dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "my_callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.05, patience=3,\n",
    "    verbose=2, baseline=0.40, start_from_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e93bf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials: 25\n"
     ]
    }
   ],
   "source": [
    "tuner_num = 4\n",
    "hp_type = \"\"\n",
    "\n",
    "base_num = 10\n",
    "layer_num = 6\n",
    "learn_num = 6\n",
    "arch_num = 3\n",
    "\n",
    "iterations = 1\n",
    "num_trials = iterations*(layer_num+learn_num+arch_num)+base_num\n",
    "\n",
    "assert base_num > 9\n",
    "print(f'Number of trials: {num_trials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34b0a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hp = create_hp(hp_type=\"base\")\n",
    "layer_hp = create_hp(hp_type=\"layer\")\n",
    "learn_hp = create_hp(hp_type=\"learn\")\n",
    "arch_hp = create_hp(hp_type=\"arch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38171e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 17s]\n",
      "val_accuracy: 0.3762102425098419\n",
      "\n",
      "Best val_accuracy So Far: 0.46565237641334534\n",
      "Total elapsed time: 00h 02m 37s\n",
      "\n",
      "Search: Running Trial #14\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |1                 |num_layers\n",
      "112               |160               |d_model\n",
      "1152              |576               |dff\n",
      "8                 |14                |num_heads\n",
      "0.225             |0.125             |dropout_rate\n",
      "7000              |6000              |warmup_steps\n",
      "32                |16                |batch_size\n",
      "0.775             |0.8275            |beta_1\n",
      "0.955             |0.96              |beta_2\n",
      "9.5557e-08        |2.0502e-08        |epsilon\n",
      "swish             |swish             |activation\n",
      "False             |False             |sequential\n",
      "\n",
      "Epoch 1/3\n",
      "134/597 [=====>........................] - ETA: 18s - loss: 2.6659 - accuracy: 0.1369"
     ]
    }
   ],
   "source": [
    "#hp_type=\"base\"\n",
    "#base_tuner = get_tuner(hp=base_hp, trials=base_num, tuner_num=tuner_num)\n",
    "\n",
    "# run 10 models to get a starting point\n",
    "#base_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             #class_weight=class_weights)\n",
    "\n",
    "# run 4 * 15 models: 6 focused on tuning layer values, 6 focused on tuning learn values, \n",
    "# and 3 focused on changing model architecture\n",
    "for i in range(1, iterations+1):\n",
    "    layer_trial = layer_num * i + base_num\n",
    "    learn_trial = learn_num * i + base_num + layer_trial\n",
    "    arch_trial  = arch_num  * i + base_num + learn_trial\n",
    "    \n",
    "    hp_type=\"layer\"\n",
    "    layer_tuner = get_tuner(hp=layer_hp, tune_new=False, trials=layer_trial, update=True, hp_type=hp_type)\n",
    "    layer_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights)\n",
    "    hp_type=\"learn\"\n",
    "    learn_tuner = get_tuner(hp=learn_hp, tune_new=False, trials=learn_trial, update=True, hp_type=hp_type)\n",
    "    learn_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights)\n",
    "    hp_type=\"arch\"\n",
    "    arch_tuner = get_tuner(hp=arch_hp, tune_new=False, trials=arch_trial, update=True, hp_type=hp_type)\n",
    "    arch_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
