{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9542c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "from sklearn.utils import class_weight\n",
    "from model_definition import *\n",
    "from tuner_trial_functions import *\n",
    "from preprocessing import vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6ee31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train_cleaned_routes', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f).values()\n",
    "#with open('./data/test_preprocessed_routes', 'rb') as f:\n",
    "#    X_test, y_test = pickle.load(f).values()\n",
    "with open('./data/val_cleaned_routes', 'rb') as f:\n",
    "    X_val, y_val = pickle.load(f).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7731334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, 0] = 3\n",
    "#X_test[:, 0] = 3\n",
    "X_val[:, 0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3440d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables:\n",
    "\n",
    "#constant\n",
    "num_classes = 9\n",
    "epochs = 40\n",
    "\n",
    "# Iteration one, 68 trials :\n",
    "\n",
    "# num_layers   =  2    -  8,   step 2   ->   2\n",
    "# d_model      =  64   -  512, step 64  ->   64\n",
    "# dff          =  512  -  2048, step 256 ->  768\n",
    "# num_heads    =  4    -  10,   step 2   ->  10\n",
    "# dropout_rate =  0.1  -  0.4,  step 0.1 ->  0.1\n",
    "# warmup_steps =  2500 -  5500, step 500 ->  3500\n",
    "\n",
    "\n",
    "\n",
    "# Iteration two, 100 trials:\n",
    "\n",
    "# num_layers   =  1    -  4,   step 1   ->  3\n",
    "# d_model      =  16   -  128, step 8  -> 128\n",
    "# dff          =  512  -  1024, step 32 -> 736\n",
    "# num_heads    =  8    -  14,   step 1   -> 8\n",
    "# dropout_rate =  0.04 -  0.24,  step 0.02 -> 0.2\n",
    "# warmup_steps =  2500 -  4750, step 250 -> 3250\n",
    "# beta_1       =  0.79 -  0.95, step 0.02 -> 0.79\n",
    "# beta_2       =  0.95 -  0.99, step 0.005 -> 0.98\n",
    "# epsilon      = 1e-11 -  1e-7, step NA   -> 6.35e-08\n",
    "\n",
    "# Iteration three, 99 trials:\n",
    "\n",
    "# num_layers   =  1    -  4,   step 1   ->  \n",
    "    # default: 2\n",
    "# d_model      =  16   -  192, step 16  -> \n",
    "    # default: 128\n",
    "# dff          =  512  -  1280, step 64 ->\n",
    "    # default: 768\n",
    "# num_heads    =  8    -  14,   step 1   -> \n",
    "    # default: 10\n",
    "# dropout_rate =  0.125 -  0.30,  step 0.025 -> \n",
    "    # default: 0.2\n",
    "# warmup_steps =  2000 -  7000, step 500 -> \n",
    "    # default: 4000\n",
    "# beta_1       =  0.74 -  0.93, step 0.0025 -> \n",
    "    # default: 0.8\n",
    "# beta_2       =  0.95 -  0.99, step 0.005 -> \n",
    "    # default: 0.98\n",
    "# epsilon      = 1e-11 -  1e-7, sample 'log' ->\n",
    "    # default: 1e-8\n",
    "# global_batch =  16   -  128,  step 16  ->\n",
    "    # default: 64\n",
    "    \n",
    "# Iteration four, 50 trials:\n",
    "\n",
    "# num_layers   =  2    -  8,   step 2   ->  \n",
    "    # default:\n",
    "# d_model      =  64   -  256, step 64  -> \n",
    "    # default:\n",
    "# dff          =  512  -  2048, step 256 ->\n",
    "    # default:\n",
    "# num_heads    =  6    -  24,   step 6   -> \n",
    "    # default:\n",
    "# dropout_rate =  0.1 -  0.4,  step 0.1 -> \n",
    "    # default:\n",
    "# warmup_steps =  2000 -  6000, step 1000 -> \n",
    "    # default:\n",
    "# batch_size =  32   -  128,  step 32  ->\n",
    "    # default:\n",
    "# lr_scalar =  0.01   -  0.13,  step 0.02  ->\n",
    "    # default:\n",
    "    \n",
    "    \n",
    "# Iteration five, 50 trials:\n",
    "\n",
    "# num_layers   =  2    -  8,   step 2   ->  \n",
    "    # default:\n",
    "# d_model      =  64   -  256, step 64  -> \n",
    "    # default:\n",
    "# dff          =  512  -  2048, step 256 ->\n",
    "    # default:\n",
    "# num_heads    =  6    -  18,   step 6   -> \n",
    "    # default:\n",
    "# dropout_rate =  0.1 -  0.4,  step 0.1 -> \n",
    "    # default:\n",
    "# warmup_steps =  2000 -  6000, step 1000 -> \n",
    "    # default:\n",
    "# batch_size =  64   -  256,  step 64  ->\n",
    "    # default:\n",
    "# lr_ramp_scalar =  0.01   -  0.13,  step 0.02  ->\n",
    "    # default:\n",
    "# lr_decay_expo =  0.01   -  0.13,  step 0.02  ->\n",
    "    # default:\n",
    "\n",
    "\n",
    "# learning_rate = transformerSchedule(d_model, warmup_steps=warmup_steps, ramp_scalar=lr_ramp_scalar, decay_expo=lr_decay_scalar)\n",
    "# optimizer = tf.keras.optimizers.Nadam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-7, use_ema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f567147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithinKAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k, name='within_k_acc', **kwargs):\n",
    "        super().__init__(name=f'within_{k}_acc', **kwargs)\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.correct_within_k = self.add_weight(name=f'correct_within_{k}', initializer='zeros')\n",
    "        self.k = k\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, dtype='float32')\n",
    "        y_pred = tf.cast(y_pred, dtype='float32')\n",
    "        abs_diff = K.abs(y_true - y_pred)\n",
    "        within = tf.cast(K.less_equal(abs_diff, self.k), dtype='float32')\n",
    "        self.total.assign_add(tf.cast(tf.size(within), dtype='float32'))\n",
    "        self.correct_within_k.assign_add(tf.reduce_sum(within))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_within_k / self.total\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.correct_within_k.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30686f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def __init__(self, num_classes, vocab_size):\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.within1_metric = WithinKAccuracy(k=1)\n",
    "        self.within2_metric = WithinKAccuracy(k=2)\n",
    "\n",
    "    def build(self, hp):\n",
    "        self.batch_size = hp.get('batch_size')\n",
    "        \n",
    "        model = EncoderClassifier(\n",
    "            num_layers=hp.get('num_layers'),\n",
    "            d_model=hp.get('d_model'),\n",
    "            num_heads=hp.get('num_heads'),\n",
    "            dff=hp.get('dff'),\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            dropout_rate=hp.get('dropout_rate'),\n",
    "            activation=hp.get('activation'),\n",
    "        )\n",
    "        \n",
    "        # Define the new learning rate schedule\n",
    "        learning_rate = transformerSchedule(hp.get('d_model'), warmup_steps=hp.get('warmup_steps'), \n",
    "                                            ramp_scalar=hp.get('lr_ramp_scalar'), decay_scalar=hp.get('lr_decay_scalar'))\n",
    "        # Update the optimizer with the new learning rate\n",
    "        if hp.get('Nadam'):\n",
    "            optimizer = tf.keras.optimizers.Nadam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-7, use_ema=hp.get('opt_special'))\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-7, amsgrad=hp.get('opt_special'))\n",
    "            \n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy']#, self.within1_metric, self.within2_metric, self.weighted_accuracy],\n",
    "        )    \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, hp, model, x, y, epochs, validation_data, class_weight=None, callbacks=None, verbose=1, **kwargs):\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        # Split the validation data into x_val and y_val\n",
    "        X_val, y_val = validation_data\n",
    "\n",
    "        # Convert the datasets to tf.data.Dataset.        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "        history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, class_weight=class_weight, \n",
    "                            callbacks=callbacks, verbose=1)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "def weighted_accuracy(self, y_true, y_pred):\n",
    "    acc_metric = self.acc_metric\n",
    "    within1_metric = self.within1_metric\n",
    "    within2_metric = self.within2_metric\n",
    "\n",
    "    # Update the metric states before calculating the metric value\n",
    "    acc_metric.update_state(y_true, y_pred)\n",
    "    acc = acc_metric.result()\n",
    "\n",
    "    within1_metric.update_state(y_true, y_pred)\n",
    "    within1 = within1_metric.result()\n",
    "\n",
    "    within2_metric.update_state(y_true, y_pred)\n",
    "    within2 = within2_metric.result()\n",
    "\n",
    "    weighted_acc = 0.3 * acc + 0.5 * within1 + 0.2 * within2\n",
    "    return weighted_acc\n",
    "\n",
    "def within1_acc(self, y_true, y_pred):\n",
    "    within1_metric = self.within1_metric\n",
    "    within1_metric.update_state(y_true, y_pred)\n",
    "    return within1_metric.result()\n",
    "\n",
    "def within2_acc(self, y_true, y_pred):\n",
    "    within2_metric = self.within2_metric\n",
    "    within2_metric.update_state(y_true, y_pred)\n",
    "    return within2_metric.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29b9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_hp(hp=kt.HyperParameters()):  #, hp_type=hp_type):  \n",
    "    #hp.Fixed(\"Tuning_Mode\", value=hp_type)\n",
    "    \n",
    "    # These Hyperparameters affect the size/complexity of the model layers\n",
    "    num_layers = hp.Int(\"num_layers\", min_value=2, max_value=8, default=2, step=2)\n",
    "    d_model = hp.Int(\"d_model\", min_value=64, max_value=256, default=128, step=64)\n",
    "    dff = hp.Int(\"dff\", min_value=512, max_value=2048, default=768, step=256)\n",
    "    num_heads = hp.Int(\"num_heads\", min_value=6, max_value=18, default=10, step=6)\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.4, default=0.2, step=0.1)\n",
    "        \n",
    "    #if(hp_type == \"learn\" or hp_type == \"base\"):\n",
    "    # These Hyperparameters affect the learning rate\n",
    "    warmup_steps = hp.Int(\"warmup_steps\", min_value=2000, max_value=6000, default=4000, step=1000)\n",
    "    batch_size = hp.Int(\"batch_size\", min_value=64, max_value=256, step=64)\n",
    "    lr_ramp_scalar = hp.Float(\"lr_ramp_scalar\", min_value=0.01, max_value=0.13, default=0.1, step=0.02)\n",
    "    lr_decay_scalar = hp.Float(\"lr_decay_scalar\", min_value=0.01, max_value=0.13, default=0.1, step=0.02)\n",
    "\n",
    "    #if(hp_type == \"arch\" or hp_type == \"base\"):\n",
    "    hp.Choice(\"activation\", values=['relu', 'selu'], default='relu')\n",
    "    hp.Boolean(\"Nadam\", default=True)\n",
    "    hp.Boolean(\"opt_special\", default=True)\n",
    "        \n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5a2c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_num = 7 #initialization\n",
    "#hp_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fab4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner(tune_new=True, trials=10, tuner_num=tuner_num):#, hp_type=hp_type):\n",
    "    #hp = create_updated_hp(tuner_num=tuner_num, hp_type=hp_type)\n",
    "    hp = create_base_hp()\n",
    "    return kt.BayesianOptimization(\n",
    "        hypermodel = MyHyperModel(9, vocab_size),\n",
    "        hyperparameters = hp,\n",
    "        #tune_new_entries = tune_new,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=trials,\n",
    "        overwrite=False,\n",
    "        directory=\"tuners\",\n",
    "        project_name=f'tuner{tuner_num}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc5edb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                  classes=np.unique(y_train),\n",
    "                                                  y=y_train) \n",
    "class_weights=dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "callbacks = tf.keras.callbacks.TensorBoard(log_dir=f'./tuner{tuner_num}logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bac3ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51 Complete [00h 04m 20s]\n",
      "val_accuracy: 0.48172324895858765\n",
      "\n",
      "Best val_accuracy So Far: 0.4973890483379364\n",
      "Total elapsed time: 00h 04m 20s\n",
      "\n",
      "Search: Running Trial #52\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "8                 |6                 |num_layers\n",
      "256               |256               |d_model\n",
      "2048              |1536              |dff\n",
      "6                 |12                |num_heads\n",
      "0.2               |0.4               |dropout_rate\n",
      "2000              |2000              |warmup_steps\n",
      "64                |64                |batch_size\n",
      "0.07              |0.07              |lr_ramp_scalar\n",
      "0.09              |0.09              |lr_decay_scalar\n",
      "relu              |relu              |activation\n",
      "True              |True              |Nadam\n",
      "False             |True              |opt_special\n",
      "\n",
      "Epoch 1/40\n",
      "212/212 [==============================] - 50s 111ms/step - loss: 2.3295 - accuracy: 0.1339 - val_loss: 2.0751 - val_accuracy: 0.1554\n",
      "Epoch 2/40\n",
      "212/212 [==============================] - 21s 97ms/step - loss: 2.0654 - accuracy: 0.1991 - val_loss: 1.6291 - val_accuracy: 0.3042\n",
      "Epoch 3/40\n",
      "212/212 [==============================] - 21s 98ms/step - loss: 1.6038 - accuracy: 0.3827 - val_loss: 1.4341 - val_accuracy: 0.4550\n",
      "Epoch 4/40\n",
      "159/212 [=====================>........] - ETA: 4s - loss: 1.4720 - accuracy: 0.4354"
     ]
    }
   ],
   "source": [
    "base_tuner = get_tuner(trials=99, tuner_num=tuner_num)\n",
    "   \n",
    "base_tuner.search(x=X_train, y=y_train, epochs=40, validation_data=(X_val, y_val), \n",
    "                  class_weight=class_weights, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375261ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('tuner_models/' + f'tuner_model_{tuner_num}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_num = 4\n",
    "hp_type = \"\"\n",
    "\n",
    "base_num = 10\n",
    "layer_num = 6\n",
    "learn_num = 6\n",
    "arch_num = 3\n",
    "\n",
    "iterations = 1\n",
    "num_trials = iterations*(layer_num+learn_num+arch_num)+base_num\n",
    "\n",
    "assert base_num >= 10\n",
    "print(f'Number of trials: {num_trials}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b0a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_hp = create_hp(hp_type=\"base\")\n",
    "#layer_hp = create_hp(hp_type=\"layer\")\n",
    "#learn_hp = create_hp(hp_type=\"learn\")\n",
    "#arch_hp = create_hp(hp_type=\"arch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38171e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iterations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#base_tuner = get_tuner(create_hp(hp_type=\"base\"), trials=base_num, tuner_num=tuner_num)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# run 10 models to get a starting point\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# run 4 * 15 models: 6 focused on tuning layer values, 6 focused on tuning learn values, \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# and 3 focused on changing model architecture\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43miterations\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     layer_trial \u001b[38;5;241m=\u001b[39m layer_num \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m base_num\n\u001b[1;32m     11\u001b[0m     learn_trial \u001b[38;5;241m=\u001b[39m learn_num \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m base_num \u001b[38;5;241m+\u001b[39m layer_trial\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iterations' is not defined"
     ]
    }
   ],
   "source": [
    "#base_tuner = get_tuner(create_hp(hp_type=\"base\"), trials=base_num, tuner_num=tuner_num)\n",
    "\n",
    "# run 10 models to get a starting point\n",
    "#base_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             #class_weight=class_weights)\n",
    "\n",
    "# run 4 * 15 models: 6 focused on tuning layer values, 6 focused on tuning learn values, \n",
    "# and 3 focused on changing model architecture\n",
    "for i in range(1, iterations+1):\n",
    "    layer_trial = layer_num * i + base_num\n",
    "    learn_trial = learn_num * i + base_num + layer_trial\n",
    "    arch_trial  = arch_num  * i + base_num + learn_trial\n",
    "    \n",
    "    \n",
    "    layer_tuner = get_tuner(tune_new=False, trials=layer_trial, tuner_num=tuner_num, hp_type='layer')\n",
    "    layer_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights)\n",
    "    \n",
    "    \n",
    "    learn_tuner = get_tuner(tune_new=False, trials=learn_trial, tuner_num=tuner_num, hp_type='learn')\n",
    "    learn_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights)\n",
    "    \n",
    "    arch_tuner = get_tuner(tune_new=False, trials=arch_trial, tuner_num=tuner_num, hp_type='arch')\n",
    "    arch_tuner.search(x=X_train, y=y_train, epochs=3, validation_data=(X_val, y_val), \n",
    "             class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0b6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e18678a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26121ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298dfa68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb92e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8579ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc309f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e774cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875af516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_updated_hp(hp=keras_tuner.HyperParameters(), hp_type=hp_type, tuner_num=tuner_num):\n",
    "    trials_np = np.zeros(0)\n",
    "    base_dir = f'tuners/tuner{tuner_num}'\n",
    "    num_trials = get_num_trials(base_dir)\n",
    "    trials_np = create_trials_np(num_trials, base_dir)\n",
    "\n",
    "    tuner_df = create_tuner_df(num_trials, trials_np)\n",
    "    df = best_trials(tuner_df, num_trials=1)    \n",
    "    \n",
    "    hp.Fixed(\"Tuning_Mode\", value=hp_type)\n",
    "           \n",
    "    if (hp_type == \"layer\" or hp_type == \"base\"):\n",
    "        # These Hyperparameters affect the size/complexity of the model layers\n",
    "        num_layers = hp.Int(\"num_layers\", min_value=1, max_value=4, default=2, step=1)\n",
    "        d_model = hp.Int(\"d_model\", min_value=16, max_value=192, default=128, step=16)\n",
    "        dff = hp.Int(\"dff\", min_value=512, max_value=1280, default=768, step=64)\n",
    "        num_heads = hp.Int(\"num_heads\", min_value=8, max_value=14, default=10, step=1)\n",
    "        dropout_rate = hp.Float(\"dropout_rate\", min_value=0.125, max_value=0.3, default=0.2, step=0.025)\n",
    "        if(hp_type != \"base\"):\n",
    "            hp.Fixed(\"warmup_steps\", value=df['warmup_steps'].iloc[0])\n",
    "            hp.Fixed(\"batch_size\", value=df['batch_size'].iloc[0])\n",
    "            hp.Fixed(\"beta_1\", value=df['beta_1'].iloc[0])\n",
    "            hp.Fixed(\"beta_2\", value=df['beta_2'].iloc[0])\n",
    "            hp.Fixed(\"epsilon\", value=df['epsilon'].iloc[0])\n",
    "\n",
    "            hp.Fixed(\"activation\", value='relu')\n",
    "            hp.Fixed(\"sequential\", value=True)\n",
    "    if (hp_type == \"learn\" or hp_type == \"base\"):\n",
    "        # These Hyperparameters affect the learning rate\n",
    "        if(hp_type != \"base\"):\n",
    "            hp.Fixed(\"num_layers\", value=df['num_layers'].iloc[0])\n",
    "            hp.Fixed(\"d_model\", value=df['d_model'].iloc[0])\n",
    "            hp.Fixed(\"dff\", value=df['dff'].iloc[0])\n",
    "            hp.Fixed(\"num_heads\", value=df['num_heads'].iloc[0])\n",
    "            hp.Fixed(\"dropout_rate\", value=df['dropout_rate'].iloc[0])\n",
    "        \n",
    "        warmup_steps = hp.Int(\"warmup_steps\", min_value=2000, max_value=7000, default=4000, step=500)\n",
    "        batch_size = hp.Int(\"batch_size\", min_value=16, max_value=128, step=16)\n",
    "        beta_1 = hp.Float(\"beta_1\", min_value=0.74, max_value=0.93, default= .8, step=0.0025)\n",
    "        beta_2 = hp.Float(\"beta_2\", min_value=0.95, max_value=0.99, default= .98, step=0.005)\n",
    "        epsilon = hp.Float(\"epsilon\", min_value=1e-9, max_value=5e-7, default= 1e-8, sampling=\"log\")\n",
    "        \n",
    "        if(hp_type != \"base\"):\n",
    "            hp.Fixed(\"activation\", value='relu')\n",
    "            hp.Fixed(\"sequential\", value=True)\n",
    "    if(hp_type == \"arch\" or hp_type == \"base\"):\n",
    "        if(hp_type != \"base\"):\n",
    "            hp.Fixed(\"num_layers\", value=df['num_layers'].iloc[0])\n",
    "            hp.Fixed(\"d_model\", value=df['d_model'].iloc[0])\n",
    "            hp.Fixed(\"dff\", value=df['dff'].iloc[0])\n",
    "            hp.Fixed(\"num_heads\", value=df['num_heads'].iloc[0])\n",
    "            hp.Fixed(\"dropout_rate\", value=df['dropout_rate'].iloc[0])\n",
    "            \n",
    "            hp.Fixed(\"warmup_steps\", value=df['warmup_steps'].iloc[0])\n",
    "            hp.Fixed(\"batch_size\", value=df['batch_size'].iloc[0])\n",
    "            hp.Fixed(\"beta_1\", value=df['beta_1'].iloc[0])\n",
    "            hp.Fixed(\"beta_2\", value=df['beta_2'].iloc[0])\n",
    "            hp.Fixed(\"epsilon\", value=df['epsilon'].iloc[0])\n",
    "        \n",
    "        hp.Choice(\"activation\", values=['relu', 'swish'], default='relu')\n",
    "        hp.Boolean(\"sequential\", default=True)\n",
    "\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b393d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0255abc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLegacyHyperModel\u001b[39;00m(\u001b[43mkeras_tuner\u001b[49m\u001b[38;5;241m.\u001b[39mHyperModel):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp):     \n\u001b[1;32m      3\u001b[0m         num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras_tuner' is not defined"
     ]
    }
   ],
   "source": [
    "class LegacyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):     \n",
    "        num_classes = 9\n",
    "        self.batch_size = hp.get('batch_size')\n",
    "       \n",
    "        model = EncoderClassifier(\n",
    "            num_layers=hp.get('num_layers'),\n",
    "            d_model=hp.get('d_model'),\n",
    "            num_heads=hp.get('num_heads'),\n",
    "            dff=hp.get('dff'),\n",
    "            vocab_size=vocab_size,\n",
    "            num_classes=num_classes,\n",
    "            dropout_rate=hp.get('dropout_rate'),\n",
    "            activation=hp.get('activation'),\n",
    "        )\n",
    "        \n",
    "        learning_rate = transformerSchedule(hp.get('d_model'), warmup_steps=hp.get('warmup_steps'), \n",
    "                                            ramp_scalar=hp.get('lr_ramp_scalar'), decay_scalar=hp.get('lr_decay_scalar'))\n",
    "        if(hp.get('Nadam')):\n",
    "            optimizer = tf.keras.optimizers.Nadam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-7, use_ema = hp.get('opt_special'))\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-7, amsgrad = hp.get('opt_special'))\n",
    "            \n",
    "            \n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy', within1_acc, within2_acc, weighted_average])    \n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, x, y, epochs, validation_data, class_weight=None, callbacks=None, verbose=1, **kwargs):\n",
    "        batch_size = self.batch_size\n",
    "                \n",
    "        # Convert the datasets to tf.data.Dataset.        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "        history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, class_weight=class_weight, \n",
    "                            callbacks=callbacks, verbose=1)\n",
    "        \n",
    "        return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
